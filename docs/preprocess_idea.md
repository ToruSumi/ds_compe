Table of Cotents
- [前処理時に検討すべきこと](#前処理時に検討すべきこと)
  - [欠損値処理](#欠損値処理)
  - [日付情報](#日付情報)
  - [ラグ特徴量](#ラグ特徴量)
  - [テキスト情報のembedding](#テキスト情報のembedding)
  - [次元圧縮](#次元圧縮)
  - [カテゴリーを持つカラムの処理](#カテゴリーを持つカラムの処理)
  - [疑似ラベリング (Pseudo Labeling)](#疑似ラベリング-pseudo-labeling)
  - [LLMのファインチューニング](#llmのファインチューニング)
  - [tf-idf](#tf-idf)
  - [類似度](#類似度)
  - [ターゲットエンコーディング](#ターゲットエンコーディング)


# 前処理時に検討すべきこと

## 欠損値処理

欠損値がある場合、以下のアプローチがある。

- 欠損値を埋める
  - 平均値、中央値、最頻値などで埋める
  - 他の特徴量を使って埋める
  - 同じようなクラスターのデータで埋める
- 欠損値を持つ行を削除する


## 日付情報

日付情報をどう取り扱うかによるが、以下のアプローチがある。（基本的には順序変数）

- 日付情報を年、月、日に分割する
  - 一定の周期性がある場合は効果的
- 日付情報を数値に変換する
- 日付情報が複数ある場合、差分なども特徴量として利用する（意味があれば）

## ラグ特徴量

時系列データの場合は過去のデータを特徴量として利用することで、モデルの精度を向上させることができる。

昨日や1週間前のデータを別のカラムに追加することで同時に過去のデータも参照することができる。

## テキスト情報のembedding

近年はLLMモデルが一般的になってきており、テキスト情報をembeddingすることが重要になってきている。

SentenceTransformerなどのモデルを用いることで、テキスト情報をembeddingすることができる。

ただし以下の注意点がある。

- 決定木系の手法において、embeddingした結果大量の特徴量になる場合有効に働かない可能性がある。

> 参考: [テーブルデータ向けの自然言語特徴抽出術](https://zenn.dev/koukyo1994/articles/9b1da2482d8ba1)

## 次元圧縮

特徴量の次元が大量にある場合、決定木系のモデルでは過学習を引き起こす可能性がある。そのため、次元圧縮を行うことで、過学習を防ぐことができる。

特にテキスト情報のembeddingを行う場合、モデルにより出力される次元数が膨大になるため、次元圧縮を行うことが重要になる。

## カテゴリーを持つカラムの処理

カテゴリーを持つカラムは、ダミー変数化を行うことで、モデルに適した形に変換することができる。
また各要素に複数のカテゴリーが含まれる場合、そのカラムを複数のカラムに分割したり、列方向にデータを分解することができる。

- 複数のカラムに分解する場合はダミー変数化を行う
  - pd.get_dummies()
  - OneHotEncoder()
  - LabelEncoder()
  - MultiLabelBinarizer()
- 列方向にデータを分解する場合は、要素を分けてデータを複製する
  - pandas.DataFrame.explode()
- 最初の要素だけ残す
  - 順序に意味がある場合などは効果がありそう
- カテゴリー数を別途追加する


## 疑似ラベリング (Pseudo Labeling)

疑似ラベリングは、モデルの予測結果をラベルとして利用する手法である。モデルの予測結果が高い確信度である場合、その予測結果をラベルとして利用することで、モデルの精度を向上させることができる。

## LLMのファインチューニング

テキスト情報をembeddingする際、LLMを用いる場合、ファインチューニングを行うことで、特定のデータに特化したembeddingを行うことができる。

PEFTなどの手法を用いることで、ファインチューニングを行うことができる。（有名なのはLoRA）

- 大本のモデルの出力形式を求めている出力に合わせるためのモデルを学習する

## tf-idf

> 出典: [tf-idfとは？](https://atmarkit.itmedia.co.jp/ait/articles/2112/23/news028.html)

テキスト情報をembeddingする際、tf-idfを用いることで、テキスト情報を数値に変換することができる。自然言語処理において、文章中に含まれる各単語が文章内でどれくらい重要かを表す統計的尺度である。

- tf: 単語の出現頻度 (term frequency)
- idf: 単語の希少性 (inverse document frequency)

td-idf値 = 出現頻度 * レア度

## 類似度

embeddingした特徴量を用いて、類似度を計算することで似ているデータを探すことができる。

- コサイン類似度（`sklearn.metrics.pairwise.cosine_similarity`）
- ユークリッド距離

## ターゲットエンコーディング

カテゴリーデータをカテゴリーに属する目的変数の平均値で置き換える。

ただしリークするおそれがあるので使用する際は注意すること。